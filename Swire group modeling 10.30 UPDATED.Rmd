---
title: "Swire Modeling"
author: "Bryson Burr, Claranne Fechter, Carl Freeze"
date: "October 27, 2025"
output:
  html_document:
    number_sections: true
    toc: true
  pdf_document:
    toc: true
editor_options:
  chunk_output_type: console
---

# Introduction

Swire Coca-Cola has begun the use of its new digital ordering platform, MyCoke360, and it has been running for a little over a year. Cart abandonment is something all businesses have to worry about, and Swire is no different. In this context, cart abandonment is described as when a customer adds items to their cart, but does not make a purchase before their next order date. This abandonment can lead to a loss in revenue. Swire has tasked us with identifying what tends to influence a customer to abandon their carts. The purpose of this notebook is to test different models that can help us understand which variables influence a cart that is abandoned and what leads customers to leaving their cart. We want to investigate what variables could influence customers and what actions they take within their carts.

# Setup 

## Package Loading

```{r package loading, message = FALSE}
# load packages
library(tidyverse)
library(dplyr)
library(skimr)
library(ggplot2)
library(janitor)
library(lubridate)
library(scales)
library(pROC)
library(caret)
library(C50)
library(xgboost)
library(glmnet)
library(ranger)
library(broom)
```

## File Loading

```{r file loading, message=FALSE}
# load all files
# clean csv's were saved from EDA notebook
customer <- read_csv("customer.csv")
cutoff_times <- read_csv("cutoff_times.csv")
ga_clean <- read_csv("ga_clean.csv")
material <- read_csv("material.csv")
operating_hours <- read_csv("operating_hours.csv")
orders_clean <- read_csv("orders_clean.csv")
sales_clean <- read_csv("sales.csv")
visits_clean <- read_csv("visits_clean.csv")
```

# Cleaning and data prep

In this notebook, we used the cleaning from our EDA and saved them as csv files, however, we have added a few things. We first combined the orders and GA tables because the orders do not match. We added the orders from the order table into the GA table to ensure the order windows could be calculated correctly. We were able to create the order windows and the target variable, which was the most complex step. We first joined the customer and cutoff tables to understand each customers policy and get each start and end date of the window. With these defined windows, we were then able to classify if the cart was abandoned or a purchase was made. We used the GA table where customers clicked add to cart but there was no purchase from the orders table, to define the cart as abandoned. After doing these crucial steps, we joined the events to the window that they belonged in to be able to see customer behavior. Lastly, we removed NAs and sampled the dataset by randomly selecting 50% of the unique customers. We chose to sample this many so that modeling would run smoother and hopefully have quicker run times. 

## Combining orders and google analytics

```{r combining datasets, eval=FALSE}
# some orders don't exist in the ga dataset get all the orders from the order dataset and add them to the ga dataset to ensure order windows can be calculated correctly
# eval = FALSE in order to speed up document knitting

# make event timestamp utc
ga_clean$event_timestamp <- ymd_hms(ga_clean$event_timestamp, tz = "UTC")
# ga clean is old source, want to keep these for any duplicates
ga_clean <- ga_clean |>
  mutate(source = "old")

# calculate order quantity to help match
ga_clean$total_quantity <- str_extract_all(ga_clean$items, '(?<="quantity":")\\d+(?:\\.\\d+)?') |>
  lapply(as.numeric) |>
  sapply(sum, na.rm = TRUE)

# collapse all orders into one line with quantity
collapsed_orders <- orders_clean |>
  group_by(customer_id, created_date_utc) |>
  reframe(total_quantity = sum(order_quantity)) |>
  ungroup()

# create dataset that can match ga_clean
orders_as_events <- collapsed_orders |>
  transmute(
    customer_id,
    event_name = "purchase",
    event_grouped = "Purchase",
    event_timestamp = ymd_hms(created_date_utc, tz = "UTC"),
    total_quantity,
    source = "new" # new source, if there are overlaps between this and ga clean, ga clean stays
  )

# get all purchases from ga clean and order dataset
ga_purchases <- ga_clean |>
  bind_rows(orders_as_events) |>
  # only want purchases
  filter(event_name == "purchase") |>
  # make event date a date
  mutate(event_date = as.Date(event_timestamp)) |>
  # keep arrange to get rid of any duplicates
  arrange(customer_id, event_date, total_quantity, desc(source)) |>
  distinct(customer_id, event_date, total_quantity, .keep_all = TRUE) |>
  # the following code is to deal with any orders that happen within a few hours of each other but are placed late enough that it processes on different days
  arrange(customer_id, event_timestamp, total_quantity, desc(source)) |>
  group_by(customer_id, total_quantity) |>
  # create event group
  # any event over 30 hours is in new group
  mutate(
    event_group = cumsum(
      difftime(event_timestamp, lag(event_timestamp, default = first(event_timestamp)), units = "hours") > 30
    )
  ) |>
  ungroup() |>
  # get rid of any duplicate orders from same event group and quantity
  distinct(customer_id, event_group, total_quantity, .keep_all = TRUE)

# add the purchases back into ga clean
ga_clean <- ga_clean |>
  # keep only non purchase events
  filter(event_name != "purchase") |>
  # adding purchases back
  bind_rows(ga_purchases) |>
  select(-c("source", "total_quantity", "event_group"))
```

## Other cleaning

```{r other cleaning}
# changing variables to correct types
customer_clean <- customer |>
  mutate(CUSTOMER_NUMBER = as.character(CUSTOMER_NUMBER))

material_clean <- material |>
  mutate(MATERIAL_ID = as.character(MATERIAL_ID))

hours_clean <- operating_hours |>
  mutate(CALLING_ANCHOR_DATE = mdy(CALLING_ANCHOR_DATE),
         CUSTOMER_NUMBER = as.character(CUSTOMER_NUMBER))
```

# Create Order Windows

```{r order windows, eval=FALSE}
# eval = FALSE in order to speed up document knitting
#function to safely generate the sequence of POSIXct dates
#creates a sequence of dates (the end times of the order windows) where start or end dates are missing
safe_seq_dates <- function(start_date, end_date, by_str) {
  if (is.na(start_date) || is.na(end_date) || start_date >= end_date) {
    return(list(as_datetime(character(0), tz="UTC")))
  }
  return(list(seq.POSIXt(from = start_date, to = end_date, by = by_str)))
}

#get start and end times
#clean cutoff times so we can join
cutoff_lookup <- cutoff_times |>
  clean_names() |>
  mutate(CUTOFF_TIME_OF_DAY = hms(cutofftime_c)) |>
  filter(sales_office != 0 & plant_id != 0) |>
  select(SALES_OFFICE = sales_office, SHIPPING_CONDITION_TIME = shipping_condition_time, 
         DISTRIBUTION_MODE = distribution_mode, CUTOFF_TIME_OF_DAY)

#join customer and cutoff times so we can see the policies
policy_periods <- visits_clean |>
  rename(CUSTOMER_NUMBER = customer_id) |>
  left_join(customer_clean |> select(CUSTOMER_NUMBER, SALES_OFFICE, DISTRIBUTION_MODE_DESCRIPTION, SHIPPING_CONDITIONS_DESCRIPTION), by = "CUSTOMER_NUMBER") |>
  left_join(cutoff_lookup, by = c("sales_office" = "SALES_OFFICE", "distribution_mode" = "DISTRIBUTION_MODE", "SHIPPING_CONDITIONS_DESCRIPTION" = "SHIPPING_CONDITION_TIME")) |>
    #make policy start time (Anchor Date + Cutoff Time)
  mutate(CUTOFF_TIME_OF_DAY = replace_na(CUTOFF_TIME_OF_DAY, hms("17:00:00")),# Default to 5 PM
         ANCHOR_DATETIME = as_datetime(anchor_date, tz="UTC") + hours(hour(CUTOFF_TIME_OF_DAY)) + 
         minutes(minute(CUTOFF_TIME_OF_DAY)) + seconds(second(CUTOFF_TIME_OF_DAY))) |>
  #get policy end time, ends when the next anchor date starts
  group_by(CUSTOMER_NUMBER) |>
  arrange(CUSTOMER_NUMBER, ANCHOR_DATETIME) |> 
  distinct(CUSTOMER_NUMBER, ANCHOR_DATETIME, .keep_all = TRUE) |> #get rid of duplicates at the same time
  mutate(POLICY_END_DATETIME = lead(ANCHOR_DATETIME), #start of next policy
         ORDER_WINDOW_START_POLICY = ANCHOR_DATETIME,
         frequency_days = frequency) |>
  ungroup() |>
  select(CUSTOMER_NUMBER, ORDER_WINDOW_START_POLICY, POLICY_END_DATETIME, frequency_days) |>
  #remove any that do not have a window or it is too short
  filter(!is.na(ORDER_WINDOW_START_POLICY), !is.na(frequency_days)) |>
  mutate(DURATION_CHECK = difftime(POLICY_END_DATETIME, ORDER_WINDOW_START_POLICY, units = "secs")) |>
  filter(is.na(POLICY_END_DATETIME) | DURATION_CHECK > 1) |> 
  select(-DURATION_CHECK)

#need to see all order windows for every row
order_window_master <- policy_periods |>
  rowwise() |> 
  mutate(max_ga_date = as_datetime(max(ga_clean$day, na.rm = TRUE) + days(max(frequency_days, na.rm=TRUE) * 2), tz="UTC"), 
         effective_order_end = coalesce(POLICY_END_DATETIME - seconds(1), max_ga_date),
         #expected end of order window
         expected_order_end_times = safe_seq_dates(start_date = ORDER_WINDOW_START_POLICY, 
                                                   end_date = effective_order_end, 
                                                   by_str = paste(frequency_days, "days"))) |>
  ungroup() |>
  unnest(cols = expected_order_end_times) |> #expand list of dates into new rows
  #actual window start date
  mutate(ORDER_WINDOW_END = expected_order_end_times, 
         ORDER_WINDOW_START = ORDER_WINDOW_END - days(frequency_days)) |>
  #unique valid windows
  select(CUSTOMER_NUMBER, ORDER_WINDOW_START, ORDER_WINDOW_END) |>
  distinct()

#get target variable
#identify windows with cart intent
cart_activity_windows <- ga_clean |>
  filter(event_name %in% c("add_to_cart", "ProductAddtoCart_PLP_Clicked", "ProductAddtoCart_PDP_Clicked")) |>
  mutate(EVENT_TIMESTAMP = ymd_hms(event_timestamp, tz="UTC")) |>
  select(CUSTOMER_ID = customer_id, EVENT_TIMESTAMP) |>
  #join cart events to the window they occurred in
  left_join(order_window_master |> rename(CUSTOMER_ID = CUSTOMER_NUMBER), 
            by = join_by(CUSTOMER_ID, 
                         closest(EVENT_TIMESTAMP >= ORDER_WINDOW_START), 
                         closest(EVENT_TIMESTAMP < ORDER_WINDOW_END))) |>
  filter(!is.na(ORDER_WINDOW_START)) |> 
  select(CUSTOMER_ID, ORDER_WINDOW_START, ORDER_WINDOW_END) |>
  distinct() |>
  mutate(HAD_CART_ACTIVITY = TRUE)

#identify windows with purchase
purchase_made_windows <- orders_clean |>
  filter(order_purchased == "Purchased") |>
  mutate(EVENT_TIMESTAMP = created_date_utc) |>
  select(CUSTOMER_ID = customer_id, EVENT_TIMESTAMP) |>
  #join purchases to the window they occurred in
  left_join(order_window_master |> rename(CUSTOMER_ID = CUSTOMER_NUMBER), 
            by = join_by(CUSTOMER_ID, 
                         closest(EVENT_TIMESTAMP >= ORDER_WINDOW_START),
                         closest(EVENT_TIMESTAMP < ORDER_WINDOW_END))) |>
  filter(!is.na(ORDER_WINDOW_START)) |>
  select(CUSTOMER_ID, ORDER_WINDOW_START, ORDER_WINDOW_END) |>
  distinct() |>
  mutate(MADE_PURCHASE = TRUE)

#create the target Variable
final_window_target <- order_window_master |>
  rename(CUSTOMER_ID = CUSTOMER_NUMBER) |>
  #merge the activity and purchase flags back onto the master list of windows
  left_join(cart_activity_windows, by = c("CUSTOMER_ID", "ORDER_WINDOW_START", "ORDER_WINDOW_END")) |>
  left_join(purchase_made_windows, by = c("CUSTOMER_ID", "ORDER_WINDOW_START", "ORDER_WINDOW_END")) |>
  #change NA to false
  mutate(HAD_CART_ACTIVITY = replace_na(HAD_CART_ACTIVITY, FALSE),
         MADE_PURCHASE = replace_na(MADE_PURCHASE, FALSE)) |>
  #target variable
  mutate(TARGET_ABANDONED = case_when(HAD_CART_ACTIVITY == TRUE & MADE_PURCHASE == FALSE ~ 1, TRUE ~ 0))

#put all together into one dataset
master_model_data <- ga_clean |>
    mutate(EVENT_TIMESTAMP = ymd_hms(event_timestamp, tz="UTC")) |>
    #join GA Events to their corresponding window
    left_join(order_window_master |> rename(CUSTOMER_ID = CUSTOMER_NUMBER),
              by = join_by(customer_id == CUSTOMER_ID,
                           closest(EVENT_TIMESTAMP >= ORDER_WINDOW_START),
                           closest(EVENT_TIMESTAMP < ORDER_WINDOW_END))) |>
    #join the target status to the events
    left_join(final_window_target |> select(CUSTOMER_ID, ORDER_WINDOW_START, ORDER_WINDOW_END, TARGET_ABANDONED),
              by = c("customer_id" = "CUSTOMER_ID", "ORDER_WINDOW_START", "ORDER_WINDOW_END")) |>
    #select columns for modeling
    select(CUSTOMER_ID = customer_id, EVENT_TIMESTAMP, event_name, event_grouped, device_category, ORDER_WINDOW_START, ORDER_WINDOW_END, TARGET_ABANDONED)
```

```{r load master model, message=FALSE}
# load master model data
master_model_data <- read_csv("master_model_data.csv")
```

## Remove NA's

```{r remove na}
# look at NA's
colSums(is.na(master_model_data))

#  get rid of any rows that don't have an order window
master_model_data_clean <- master_model_data |>
  filter(!is.na(TARGET_ABANDONED))
```

# Modeling setup

## Sample Data

```{r sample data}
#sample data for quicker run times
set.seed(123)

#going to sample 50% of unique customers
sample_ids <- master_model_data_clean |>
  distinct(CUSTOMER_ID)  |>
  slice_sample(prop = 0.5)  |>
  pull(CUSTOMER_ID)

# keep only those randomly sampled customers
master_model_data_clean_sample <- master_model_data_clean  |>
  filter(CUSTOMER_ID %in% sample_ids)

#adding in a purchase indicating variable
master_model_data_clean_sample <- master_model_data_clean_sample |>
  mutate(is_purchase_event = ifelse(event_grouped == "Purchase", 1, 0))

# turn any na's in device category to null
master_model_data_clean_sample <- master_model_data_clean_sample |>
  mutate(device_category = ifelse(is.na(device_category), "null", device_category))
```

## Classifier split

```{r classifier split}
# see sample classifier split
mean(master_model_data_clean_sample$TARGET_ABANDONED)

table(master_model_data_clean_sample$TARGET_ABANDONED)
```

## Train and test sets

```{r train and test}
# get all unique customer id's for sampling
all_customer_ids <- master_model_data_clean_sample |>
  distinct(CUSTOMER_ID) |>
  pull(CUSTOMER_ID) 

# get sample of customer id's to be used in training fold
index <- sample(x = all_customer_ids, size = floor(length(all_customer_ids) * 0.70), replace = FALSE)

# Subset train using index to create a 70% train_fold
train_fold <- master_model_data_clean_sample |> 
  filter(CUSTOMER_ID %in% index)

# Subset the remaining rows not included in index to create a 30% validation fold
validation_fold <- master_model_data_clean_sample |> 
  filter(!(CUSTOMER_ID %in% index))

#large imbalance so going to down sample
table(train_fold$TARGET_ABANDONED)

#have to make factor 
train_fold <- train_fold |>
  mutate(TARGET_ABANDONED = factor(TARGET_ABANDONED))
validation_fold <- validation_fold |>
  mutate(TARGET_ABANDONED = factor(TARGET_ABANDONED))

# getting event names so training and validation levels can match
train_events_char <- as.character(train_fold$event_name)
validation_events_char <- as.character(validation_fold$event_name)

full_levels_events <- sort(unique(c(train_events_char, validation_events_char)))

# apply all factor leveles to training data
train_fold$event_name <- factor(train_events_char, 
                                levels = full_levels_events)

## apply all factor leveles to validation data
validation_fold$event_name <- factor(validation_events_char, 
                                     levels = full_levels_events)

# make device category a factor
train_fold <- train_fold |>
  mutate(device_category = factor(device_category))
validation_fold <- validation_fold |>
  mutate(device_category = factor(device_category))

# downsampling due to highly imbalanced distribution
train_fold <- downSample(x = train_fold |> select(-TARGET_ABANDONED), y = train_fold$TARGET_ABANDONED)

# rename class to target abandoned
train_fold <- train_fold |> rename(TARGET_ABANDONED = Class)

# quick check on distribution
table(train_fold$TARGET_ABANDONED)
```

# Models

##  Load Models

```{r load models}
# models previously ran to decrease document knitting time
# load models 
load("models.RData")
```

## Performance Benchmark

```{r performance benchmark}
# probability of abandoned
p_yes <- mean(train_fold$TARGET_ABANDONED == 1)

# Predict 1 with probability p_yes
random_pred <- rbinom(nrow(validation_fold), size = 1, prob = p_yes)

# Evaluate accuracy
mean(random_pred == validation_fold$TARGET_ABANDONED)

# Evaluating AUC
roc_random <- roc(validation_fold$TARGET_ABANDONED, random_pred)
auc(roc_random)
```

This provides a benchmark for our models. A random classifier is around 50% accurate
with an AUC score of .5. This is just a random guess with no predictive power.

## Logistic Regression

```{r event name train, eval = FALSE}
# basic logistic regression with event name
basic_regression_name <- glm(TARGET_ABANDONED ~ event_name + device_category, data = train_fold, family = binomial)
```

```{r event name results}
# look at summary, kept 
tidy(basic_regression_name) |>
  head(10)
```

```{r event group train, eval= FALSE}
#basic logistic regression with event grouped
basic_regression_group <- glm(TARGET_ABANDONED ~ event_grouped + device_category, data = train_fold, family = binomial)
```

```{r event group results}
tidy(basic_regression_group) |>
  head(10)

# predict probabilities
group_prob <- predict(basic_regression_group, newdata = validation_fold, type = "response")

# get AUC score
auc_value <- roc(validation_fold$TARGET_ABANDONED, group_prob)
auc_value$auc

# find best threshold
thr <- as.numeric(coords(auc_value, "best", ret = "threshold"))

# Convert probabilities to predicted classes
pred_class <- ifelse(group_prob > thr, "1", "0")

# factor with same levels as validation fold
pred_class <- factor(pred_class, levels = levels(validation_fold$TARGET_ABANDONED))

# Confusion matrix
confusionMatrix(pred_class, validation_fold$TARGET_ABANDONED)
```

```{r interaction train, eval=FALSE}
#added interaction
interaction_regression <- glm(TARGET_ABANDONED ~ event_grouped + device_category * is_purchase_event, data = train_fold, family = binomial)
```

```{r interaction results}
tidy(interaction_regression) |>
  head(10)

# predict probabilities
inter_prob <- predict(interaction_regression, newdata = validation_fold, type = "response")

# get AUC score
auc_inter <- roc(validation_fold$TARGET_ABANDONED, inter_prob)
auc_inter$auc

# find best threshold
thr <- as.numeric(coords(auc_inter, "best", ret = "threshold"))

# Convert probabilities to predicted classes
pred_inter <- ifelse(inter_prob > thr, "1", "0")

# factor with same levels as validation fold
pred_inter <- factor(pred_inter, levels = levels(validation_fold$TARGET_ABANDONED))

# Confusion matrix
confusionMatrix(pred_inter, validation_fold$TARGET_ABANDONED)
```

### Summarized user actions logistic regression

```{r log summarized prep}
# Rebuild minimal behavior features
if (!exists("behavior_features") || nrow(behavior_features) == 0L) {
  
  mm <- master_model_data_clean_sample |>
    mutate(event_name_norm = tolower(trimws(event_name))) |>
    arrange(CUSTOMER_ID, ORDER_WINDOW_START, EVENT_TIMESTAMP)
  
  behavior_features <- mm |>
    group_by(CUSTOMER_ID, ORDER_WINDOW_START, ORDER_WINDOW_END, TARGET_ABANDONED) |>
    summarise(
      total_events = n(),
      add_to_cart_count = sum(event_name_norm == "add_to_cart", na.rm = TRUE),
      remove_count = sum(event_name_norm == "remove_from_cart", na.rm = TRUE),
      update_cart_count = sum(event_name_norm == "update_cart", na.rm = TRUE),
      view_item_count = sum(event_name_norm == "view_item", na.rm = TRUE),
      view_list_count = sum(event_name_norm == "view_item_list", na.rm = TRUE),
      begin_ck_count = sum(event_name_norm == "begin_checkout", na.rm = TRUE),
      proceed_ck_count = sum(event_name_norm == "proceed_to_checkout", na.rm = TRUE),
      idx_add = {w <- which(event_name_norm == "add_to_cart"); if (length(w)) w[1] else NA_integer_},
      idx_begin = {w <- which(event_name_norm == "begin_checkout"); if (length(w)) w[1] else NA_integer_ },
      idx_proceed = {w <- which(event_name_norm == "proceed_to_checkout"); if (length(w)) w[1] else NA_integer_},
      idx_remove = {w <- which(event_name_norm == "remove_from_cart"); if (length(w)) w[1] else NA_integer_ },
      .groups = "drop"
    ) |>
    mutate(
      made_checkout_progress   = as.integer(!is.na(idx_begin) | !is.na(idx_proceed)),
      remove_after_add         = as.integer(!is.na(idx_add) & !is.na(idx_remove) & idx_remove > idx_add),
      browse_heavy_no_checkout = as.integer((view_item_count + view_list_count) >= 5 & made_checkout_progress == 0)
    ) |>
    select(-idx_add, -idx_begin, -idx_proceed, -idx_remove) |>
    ungroup()
}

bf <- behavior_features |>
  filter(!is.na(TARGET_ABANDONED)) |>
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .)))

all_customer_ids <- bf |>
  distinct(CUSTOMER_ID) |>
  pull(CUSTOMER_ID)

train_ids <- sample(all_customer_ids, size = floor(length(all_customer_ids) * 0.7), replace = FALSE)

# Create train/validation folds by filtering aggregated features
train_fold_bf <- bf |> filter(CUSTOMER_ID %in% train_ids)
validation_fold_bf <- bf |> filter(!(CUSTOMER_ID %in% train_ids))
```

```{r log summarized model, eval=FALSE}
logit_model <- glm(
  TARGET_ABANDONED ~
    add_to_cart_count + remove_count + update_cart_count +
    view_item_count + view_list_count +
    begin_ck_count +
    browse_heavy_no_checkout +
    remove_after_add +
    total_events,
  data = train_fold_bf, family = binomial)
```

```{r log summarized results}
# predict probabilities
prob <- predict(logit_model, newdata = validation_fold_bf, type = "response")
# create roc object
roc_obj <- roc(validation_fold_bf$TARGET_ABANDONED, prob)
# find best threshold
thr <- as.numeric(coords(roc_obj, "best", ret = "threshold"))
# convert to predictions
pred <- ifelse(prob >= thr, 1L, 0L)

# Metrics
accuracy <- mean(pred == validation_fold_bf$TARGET_ABANDONED)
auc_val  <- as.numeric(auc(roc_obj))

# Top decile lift to see how concentrated are abandons in the riskiest 10%?
cut90        <- quantile(prob, 0.90, na.rm = TRUE)
overall_rate <- mean(validation_fold_bf$TARGET_ABANDONED)
top10_rate   <- mean(validation_fold_bf$TARGET_ABANDONED[prob >= cut90])
lift_pp      <- 100 * (top10_rate - overall_rate)

# Confusion matrix at threshold
TP <- sum(pred == 1 & validation_fold_bf$TARGET_ABANDONED == 1)
TN <- sum(pred == 0 & validation_fold_bf$TARGET_ABANDONED == 0)
FP <- sum(pred == 1 & validation_fold_bf$TARGET_ABANDONED == 0)
FN <- sum(pred == 0 & validation_fold_bf$TARGET_ABANDONED == 1)

metrics_tbl <- data.frame(
  Threshold = round(thr, 3),
  Accuracy  = round(accuracy, 3),
  AUC       = round(auc_val, 3),
  Overall_Abandon_Rate = round(overall_rate, 3),
  TopDecile_Abandon_Rate = round(top10_rate, 3),
  Lift_pp   = round(lift_pp, 1),
  TP = TP, FP = FP, TN = TN, FN = FN)
print(metrics_tbl)

# Make coefficients easy to read with odds ratios
# Greater than 1 increases odds, less decreases
coefs <- summary(logit_model)$coefficients
interpret <- data.frame(
  Feature   = rownames(coefs),
  OddsRatio = round(exp(coefs[, "Estimate"]), 3),
  PValue    = round(coefs[, "Pr(>|z|)"], 4),
  row.names = NULL) |> 
  arrange(desc(abs(log(OddsRatio))))
  print(head(interpret, 12))

# Plot ROC
print(plot(roc_obj, main = paste0("ROC (AUC = ", round(auc_val, 3), ")")))

# Plot lift bars to compare abandonrate between overall and top 10% highest risk windows
lift_df <- data.frame(
  Segment = c("Overall", "Top 10% by Risk"),
  Rate    = c(overall_rate, top10_rate))

p_lift <- ggplot(lift_df, aes(x = Segment, y = Rate)) +
    geom_col() +
    geom_text(aes(label = percent(Rate, accuracy = 0.1)), vjust = -0.2) +
    ylim(0, max(lift_df$Rate) * 1.15) +
    labs(title = "Abandonment Rate: Overall vs. Top-Decile", y = "Abandonment Rate", x = "") +
    theme_minimal()
  print(p_lift)

# Top 10 odds ratio bars to see which behaviors push risk up and down the most
or_plot_df <- interpret |>
  filter(Feature != "(Intercept)") |>
  mutate(
    Direction = ifelse(OddsRatio >= 1, "Increases Odds", "Decreases Odds"),
    Feature   = gsub("_", " ", Feature)
  ) |>
  arrange(desc(abs(log(OddsRatio)))) |>
  head(10)

p_or <- ggplot(or_plot_df, aes(x = reorder(Feature, log(OddsRatio)), y = log(OddsRatio), fill = Direction)) +
  geom_col() +
  coord_flip() +
  labs(title = "Top Behavioral Drivers (log Odds Ratio)",
        x = "", y = "log(OR)  (positive = higher abandonment risk)") +
  theme_minimal() +
  theme(legend.position = "bottom")
print(p_or)
```

## Decision Tree

```{r decision tree train, eval=FALSE}
dec_tree <- C5.0(TARGET_ABANDONED ~ event_name + device_category, data = train_fold)
```

```{r decision tree results}
tree_preds <- predict(dec_tree, newdata = validation_fold, type = "prob")[,2]

# get AUC score
auc_dec <- roc(validation_fold$TARGET_ABANDONED, tree_preds)
auc_dec$auc

# find best threshold
thr <- as.numeric(coords(auc_dec, "best", ret = "threshold"))

# Convert probabilities to predicted classes
pred_dec <- ifelse(tree_preds > thr, "1", "0")

# factor with same levels as validation fold
pred_dec <- factor(pred_dec, levels = levels(validation_fold$TARGET_ABANDONED))

# Confusion matrix
confusionMatrix(pred_dec, validation_fold$TARGET_ABANDONED)
```

```{r decision tree trials, eval=FALSE}
dec_tree_2 <- C5.0(TARGET_ABANDONED ~ event_name + device_category, data = train_fold, trials = 3)
```

```{r trials results}
tree_preds_trial <- predict(dec_tree_2, newdata = validation_fold, type = "prob")[,2]

# get AUC score
auc_trials <- roc(validation_fold$TARGET_ABANDONED, tree_preds_trial)
auc_trials$auc

# find best threshold
thr <- as.numeric(coords(auc_trials, "best", ret = "threshold"))

# Convert probabilities to predicted classes
pred_trial <- ifelse(tree_preds_trial > thr, "1", "0")

# factor with same levels as validation fold
pred_trial <- factor(pred_trial, levels = levels(validation_fold$TARGET_ABANDONED))

# Confusion matrix
confusionMatrix(pred_trial, validation_fold$TARGET_ABANDONED)
```

## Random Forest

```{r random forest train, eval=FALSE}
#Random forest model
# fit single model with RF as the base learner
rf_mod <- train(TARGET_ABANDONED ~ event_grouped + device_category,
                data = train_fold_caret,
                method = "ranger",
                metric = "ROC",
                trControl = train_control,
                num.trees = 250,
                importance = 'impurity')
```

```{r rf results}
print(rf_mod$results)

importance_rf <- varImp(rf_mod, scale = FALSE)
print(importance_rf)

rf_preds <- predict(rf_mod, newdata = validation_fold, type = "prob")[,2]

# get AUC score
auc_rf <- roc(validation_fold$TARGET_ABANDONED, rf_preds)
auc_rf$auc

# find best threshold
thr <- as.numeric(coords(auc_rf, "best", ret = "threshold"))

# Convert probabilities to predicted classes
pred_rf <- ifelse(rf_preds > thr, "1", "0")

# factor with same levels as validation fold
pred_rf <- factor(pred_rf, levels = levels(validation_fold$TARGET_ABANDONED))

# Confusion matrix
confusionMatrix(pred_rf, validation_fold$TARGET_ABANDONED)
```

### Summarized user actions random forest

```{r rf summarized prep}
# Ranger expects the target as a factor for classification
train_fold_bf$TARGET_ABANDONED <- factor(train_fold_bf$TARGET_ABANDONED, levels = c(0,1))
validation_fold_bf$TARGET_ABANDONED  <- factor(validation_fold_bf$TARGET_ABANDONED,  levels = c(0,1))

rf_formula <- TARGET_ABANDONED ~
  add_to_cart_count + remove_count + update_cart_count +
  view_item_count + view_list_count +
  begin_ck_count + proceed_ck_count +
  browse_heavy_no_checkout + remove_after_add +
  total_events

# Train random forest with 300 trees 
p <- ncol(model.matrix(rf_formula, data = train_fold_bf)) - 1
```

```{r rf summarized model, eval=FALSE}
rf_model <- ranger(
  rf_formula,
  data = train_fold_bf,
  num.trees = 300,
  mtry = floor(sqrt(p)),
  min.node.size = 20,
  probability = TRUE,          
  importance = "impurity",    
  seed = 123)
```

```{r rf summarized results}
# Get abandonment probabilities on the validation set 
rf_probs <- predict(rf_model, data = validation_fold_bf)$predictions[, "1"]   
y_true   <- as.integer(as.character(validation_fold_bf$TARGET_ABANDONED))     

# Metrics at simple 0.5 cutoff 
thr     <- 0.5
rf_pred <- ifelse(rf_probs >= thr, 1L, 0L)
acc     <- mean(rf_pred == y_true)

# AUC to measure ranking quality 
roc_obj <- roc(y_true, rf_probs)
auc_val <- as.numeric(auc(roc_obj))

# Confusion matrix pieces + precision/recall
TP <- sum(rf_pred == 1 & y_true == 1)
TN <- sum(rf_pred == 0 & y_true == 0)
FP <- sum(rf_pred == 1 & y_true == 0)
FN <- sum(rf_pred == 0 & y_true == 1)

metrics_tbl <- data.frame(
  Accuracy  = round(acc, 3),
  AUC       = round(auc_val, 3),
  Precision = round(ifelse((TP+FP)==0, NA, TP/(TP+FP)), 3),
  Recall    = round(ifelse((TP+FN)==0, NA, TP/(TP+FN)), 3),
  TP = TP, FP = FP, TN = TN, FN = FN
)
print(metrics_tbl)

#  Top-decile lift and find abandonment rate vs overall on 10% riskiest windows
cut90   <- quantile(rf_probs, 0.90, na.rm = TRUE)
overall <- mean(y_true)                       
top10   <- mean(y_true[rf_probs >= cut90])     
lift_pp <- 100 * (top10 - overall)              
lift_tbl <- data.frame(
  Overall_Rate = round(overall, 3),
  Top10_Rate   = round(top10, 3),
  Lift_pp      = round(lift_pp, 1)
)
print(lift_tbl)

# Show top 10 behaviors the forest leaned on
imp <- sort(rf_model$variable.importance, decreasing = TRUE)
imp_df <- data.frame(Feature = names(imp), Importance = as.numeric(imp), row.names = NULL) |> head(10)

p_imp <- ggplot(imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col() +
  coord_flip() +
  labs(title = "Random Forest — Top Feature Importance",
       x = "", y = "Importance") +
  theme_minimal()
print(p_imp)

# ROC curve
print(plot(roc_obj, main = paste0("Random Forest ROC (AUC = ", round(auc_val, 3), ")")))
```

## XGBoost

```{r xgboost prep}
#xgboost 
predictors_train <- train_fold_caret |> 
  select(event_grouped, device_category, is_purchase_event)

dummy <- dummyVars(~ ., data = predictors_train)

train_matrix <- predict(dummy, newdata = predictors_train)


xgb_tune_grid <- expand.grid(nrounds = c(50, 150, 300),
                             max_depth = c(2, 4),
                             eta = c(0.1, 0.3),
                             gamma = 0,
                             colsample_bytree = 0.8,
                             min_child_weight = 1,
                             subsample = 0.8)
```

```{r xgboost model, eval=FALSE}
xgb_mod <- train(x = train_matrix, 
                 y = train_fold_caret$TARGET_ABANDONED,
                 method = "xgbTree",
                 metric = "ROC",
                 trControl = train_control,
                 tuneGrid = xgb_tune_grid)
```

```{r xgboost results}
print(xgb_mod$results)

#find important features
importance_xgb <- varImp(xgb_mod, scale = FALSE)
print(importance_xgb)
```

## Lasso

```{r lasso model, eval=FALSE}
#lasso
lasso_model <- train(x = train_matrix,
                     y = train_fold_caret$TARGET_ABANDONED,
                     method = "glmnet",
                     metric = "ROC",
                     trControl = train_control,
                     tuneGrid = expand.grid(alpha = 1, lambda = 10^seq(10, -2, length=100))) #alpha = 1 for lasso
```

```{r lasso results}
# showing top 10 to avoid lengthy output
head(lasso_model$results, 10)

#find most important features
best_lambda <- lasso_model$bestTune$lambda
lasso_coefficients <- coef(lasso_model$finalModel, s = best_lambda)
influential_events <- data.frame(Feature = rownames(lasso_coefficients), 
                                 Coefficient = as.vector(lasso_coefficients)) |>
  arrange(desc(abs(Coefficient)))
print(influential_events)
```

```{r save models, eval = FALSE}
# save models for future loading to reduce knitting time
save(basic_regression_name, basic_regression_group, interaction_regression, logreg,logit_model, dec_tree, dec_tree_2, rf_mod, rf_model, xgb_mod, xgb_mod, lasso_model, file = "models.RData")
```


# Modeling Process

The goal of our modeling was to identify which customer behaviors most strongly predict cart abandonment. Using the order window logic from our EDA, we defined an abandoned cart as any window where a customer added items but didn’t purchase before their cutoff time. We merged Google Analytics, orders, and visit-plan data to align all events under this framework and then sampled about half of unique customers to manage the runtime. The data was split 80/20 by customer so no individual appeared in both training and validation sets.

We began with a random baseline model which performed with roughly a 58% accuracy and an AUC of 0.50. This essentially shows similar to results if we guessed randomly. From there, we built several models to test different representations of user behavior.
The first approach used event data such as event_name, event_grouped, and device_category, but this version underperformed with an AUC of about 0.54. The event level introduced too much noise where many events were infrequent or overlapped in meaning.
To address this, we created summarized behavioral features that better captured customer intent on the site. Examples include the number of items added to cart, cart updates, product views, and checkout attempts. We also included key behavioral flags like “browsed heavily without starting checkout” and “removed items after adding.” These metrics condensed hundreds of events into meaningful indicators of purchase intent and any hesitation the customers had.

With these features, a logistic regression achieved a strong AUC of 0.895. The model cleanly separated abandoned from completed order windows and produced useful results. Customers who removed items or browsed heavily without checking out were far more likely to abandon, while those who started checkout had a much lower risk.

We then tested nonlinear models to capture interactions among the behaviors. A decision tree was interpretable but still limited with an AUC of about 0.54. The random forest performed best, reaching an AUC of 0.98 and accuracy of about 88%, confirming that ensembles could capture insightful combinations of behaviors. XGBoost also performed very well, achieving similar AUC and identifying the same high-impact features, showing strong consistency across model types. The LASSO regression added further validation by highlighting the same predictors while automatically shrinking less useful variables, improving model simplicity without hurting accuracy.

Overall, the summarized behavior models were by far the most effective. Logistic regression provided interpretability, random forest delivered the best predictive performance, and XGBoost and LASSO confirmed the reliability of the behavioral patterns driving cart abandonment.

# Model Performance

Most of our models, that were simple, produced an AUC score slightly higher than our performance benchmark of 0.500, however, there were two that stood out. Both models that used feature engineering had much higher AUC scores. 

The summarized user actions logistic regression had an AUC of 0.895. This high AUC means the model was able to rank order windows by their likelihood of abandonment. We found a strong lift ratio of 4.8 between the top 10% versus the overall rate. We were able to gain valuable insights about what behaviors play a role in cart abandonment. The strongest predictor was browsing without starting the checkout process which had an odds ratio of 26.4 and the next predictor was removing an item after adding one, with an odds of 4.5. This shows customer indecision leads to cart abandonment the most.

The summarized user actions random forest had an AUC of 0.979, which was the highest. This model had a lift ratio of 8.9 between the top 10% and the overall. This would mean targeting the top 10% of high risk windows/customers would capture almost 9 times as many abandoned carts as it would if we just randomly selected them. We achieve as precision of 79% which is how many true positives were flagged. With random forest it is harder to gain understanding about individual features. We made a list of the most important features to the model and the top ones were count of add to cart, begin checkout, total events, counts of update cart, and browsing but not start checkout. These show how much these predictors contributed to the models output, so not necessarily predicting for cart abandonment. This model does not help us with truly determining which behaviors lead to cart abandonment. 

Lastly, we want to mention that both of these AUC scores suggest excellent predicting power, but could be suspiciously high. These scores could possibly be inflated due to being overfit or a having data leakage. We would like to explore this idea further. 

# Results

The results from our models show that cart abandonment is strongly linked to specific behavioral patterns rather than random user activity. The behavioral feature engineering was the key turning point that made all models much more accurate and interpretable.

The logistic regression provided a clear and easy to understand view of customer behavior. Customers who browsed heavily without starting checkout or removed items after adding them were the most likely to abandon their carts. On the other hand, those who initiated checkout were far less likely to abandon. This model had an AUC of 0.895 showing strong results to separate abandoned sessions from completed ones.

The random forest had the best predictive performance with an AUC of 0.98 and accuracy of about 88%. It was able to identify complex relationships for the behaviors that the logistic regression could not fully capture. The top predictors were consistent across both models heavy browsing without checkout, cart removals, and frequent cart updates were the clearest warning signs.

The XGBoost model confirmed these same results, producing nearly identical rankings of the most important features. It reinforced the stability of our findings, while the LASSO regression validated them by automatically filtering out weaker predictors.
Overall, the models point to clear opportunities for Swire to intervene earlier. Customers who view items multiple times, update their carts, or remove products without checking out should be prioritized for reminders or promotions to reduce abandonment and increase the conversion rate.


# Group Contribution

Carl - Created a logistic regression and random forest model in my individual r file to help be used as a reference for the group code. Completed the modeling process and results writing section.

Claranne - Created basic logistic regression, random forest, xgboost, and lasso models. Also worked to create the order window and target variable. Completed introduction, data preparation, and model performance writing sections. 

Bryson - 
